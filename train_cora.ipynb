{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "static-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN related libraries\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys \n",
    "sys.path.append('./')\n",
    "\n",
    "# from the code \n",
    "from model.GAT import GAT\n",
    "from utils.layers import GAT_layer\n",
    "\n",
    "# data related \n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ranking-punishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beautiful-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index=dataset[0].edge_index\n",
    "nodes_features=dataset[0].x\n",
    "nodes_labels=dataset[0].y\n",
    "\n",
    "#parameters_GAT_network={'num_features_per_layer'={}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3fdab0",
   "metadata": {},
   "source": [
    "### Bellow we just test the GAT_layer to check that is returning what we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b734c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_test=GAT_layer(nodes_features.shape[0],nodes_features.shape[1],8,8,nn.ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "opened-mixer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1441,  0.0197, -0.0162,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0667,  0.1129, -0.0322,  ...,  0.0610,  0.0526, -0.0135],\n",
       "         [ 0.0410,  0.0206, -0.0472,  ...,  0.0064, -0.0332, -0.0613],\n",
       "         ...,\n",
       "         [-0.0078,  0.1516, -0.0268,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.5963,  0.0873, -0.3285,  ..., -0.0198,  0.1006, -0.1098],\n",
       "         [ 0.0622,  0.2269, -0.0720,  ..., -0.0416,  0.0839,  0.1467]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
       "         [ 633, 1862, 2582,  ...,  598, 1473, 2706]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_test((nodes_features,edge_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04cdd3",
   "metadata": {},
   "source": [
    "### Bellow we test the GAT network. We put two layers, similar to the model used in the experiment in the original paper, which is the one we'll use latter for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75a47fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test=GAT(2,nodes_features.shape[0],[nodes_features.shape[1],8,7],[8,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90c59562",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_features=model_test((nodes_features,edge_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79f7b9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1758, 0.2000, 0.1018, 0.1263, 0.1113, 0.1947, 0.0901],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_features[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f65036f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9661, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check that the output has the right shape to pass to the loss function\n",
    "loss=nn.CrossEntropyLoss()\n",
    "loss(out_features[0],nodes_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ea7a1a",
   "metadata": {},
   "source": [
    "## Training the network in the cora dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9856ba",
   "metadata": {},
   "source": [
    "### First we define the hyperparameters of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146396d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "C=7 # number of classes of the cora dataset\n",
    "params_network={'num_layers':2,\n",
    "               'num_nodes':nodes_features.shape[0],\n",
    "                'num_features_per_layer':[nodes_features.shape[1],8,C],\n",
    "                'num_heads_per_layer':[8,1],\n",
    "                 'num_epochs':500\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ff5f1",
   "metadata": {},
   "source": [
    "### Divide the dataset in training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b440cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset starts in node 0 and comprises 140 nodes\n",
      "The validation dataset starts in node 1708 and comprises 1000 nodes\n",
      "The test dataset starts in node 140 and comprises 500 nodes\n"
     ]
    }
   ],
   "source": [
    "# indices of each set according to the masks given in the dataset (i.e we use the same assignation as in the original paper)\n",
    "training_set_indices=(dataset[0].train_mask).nonzero(as_tuple=False).flatten()\n",
    "test_set_indices=(dataset[0].test_mask).nonzero(as_tuple=False).flatten()\n",
    "val_set_indices=(dataset[0].val_mask).nonzero(as_tuple=False).flatten()\n",
    "print('The training dataset starts in node {:} and comprises {:} nodes'.format(training_set_indices[0].numpy(),training_set_indices.shape[0]))\n",
    "print('The validation dataset starts in node {:} and comprises {:} nodes'.format(test_set_indices[0].numpy(),test_set_indices.shape[0]))\n",
    "print('The test dataset starts in node {:} and comprises {:} nodes'.format(val_set_indices[0].numpy(),val_set_indices.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27a9beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the labels for the training set\n",
    "nodes_labels_training_set=nodes_labels.index_select(0,training_set_indices)\n",
    "#validation\n",
    "nodes_labels_validation_set=nodes_labels.index_select(0,val_set_indices)\n",
    "#test\n",
    "nodes_labels_test_set=nodes_labels.index_select(0,test_set_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440ede0",
   "metadata": {},
   "source": [
    "Now we have everything we need in order to start the training process. Let's define the model and run the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34d41ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GAT(params_network['num_layers'],params_network['num_nodes'],params_network['num_features_per_layer'],params_network['num_heads_per_layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9c92d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the training loop \n",
    "def train_gat(params_network,num_epochs=10000,val_lapse=1000,perform_test='True'):\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device=torch.device('mps')\n",
    "    time_start=time.time()\n",
    "    model_gat=GAT(params_network['num_layers'],params_network['num_nodes'],params_network['num_features_per_layer'],params_network['num_heads_per_layer']).to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model_gat.parameters(), lr=0.01,weight_decay=0.0005) # weight decay corresponds to the L2 penalty, which in the original implementation is chosen to the value we put here\n",
    "    \n",
    "    graph_data=(nodes_features,edge_index)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)\n",
    "        \n",
    "        model_gat.train() #set model in training mode\n",
    "        \n",
    "        # We do a forward pass of the model and extract the unnormalized logits for the training set \n",
    "        # shape = (N, C) where N is the number of nodes in the split (train/val/test) and C is the number of classes\n",
    "        nodes_unnormalized_out_train = model_gat(graph_data)[0].index_select(0,training_set_indices)\n",
    "        \n",
    "        loss=loss_fn(nodes_unnormalized_out_train,nodes_labels_training_set)\n",
    "        \n",
    "        #Optimizer backward evaluation\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        \n",
    "        # Compute the accuracy\n",
    "\n",
    "        # Finds the index of maximum (unnormalized) score for every node and that's the class prediction for that node.\n",
    "        # Compare those to true (ground truth) labels and find the fraction of correct predictions -> accuracy metric.\n",
    "        predictions = torch.argmax(nodes_unnormalized_out_train, dim=-1)\n",
    "        accuracy = torch.sum(torch.eq(predictions, nodes_labels_training_set).long()).item() / len(nodes_labels_training_set)\n",
    "        \n",
    "        #TensorBoard summary writter \n",
    "        writer=SummaryWriter()\n",
    "        \n",
    "        writer.add_scalar('Loss/train',loss.item(),epoch)\n",
    "        writer.add_scalar('Accuracy/train',accuracy,epoch)\n",
    "        \n",
    "        print(f'time elapsed={(time.time()-time_start):.2f} [s]')\n",
    "        print(f'accuracy test={accuracy:.3f}')\n",
    "        if (epoch+1)%val_lapse==0:\n",
    "            with torch.no_grad():\n",
    "                nodes_unnormalized_out_val = model_gat(graph_data)[0].index_select(0,val_set_indices)\n",
    "                loss_val=loss_fn(nodes_unnormalized_out_val,nodes_labels_validation_set)\n",
    "                predictions = torch.argmax(nodes_unnormalized_out_val, dim=-1)\n",
    "                accuracy = torch.sum(torch.eq(predictions, nodes_labels_validation_set).long()).item() / len(nodes_labels_validation_set)\n",
    "        \n",
    "                \n",
    "                writer.add_scalar('Loss/validation',loss_val.item(),epoch)\n",
    "                writer.add_scalar('Accuracy/validation',accuracy,epoch)\n",
    "                print(f'GAT training: time elapsed= {(time.time() - time_start):.2f} [s] | epoch={epoch + 1} | val acc={accuracy}')\n",
    "        \n",
    "    if perform_test:\n",
    "        with torch.no_grad():\n",
    "            nodes_unnormalized_out_test = model_gat(graph_data)[0].index_select(0,test_set_indices)\n",
    "            loss_test=loss_fn(nodes_unnormalized_out_test,nodes_labels_test_set)\n",
    "            predictions = torch.argmax(nodes_unnormalized_out_test, dim=-1)\n",
    "            accuracy = torch.sum(torch.eq(predictions, nodes_labels_test_set).long()).item() / len(nodes_labels_test_set)\n",
    "        print(f'Test accuracy = {test_acc}')\n",
    "    torch.save({'state_dict':model.state_dict},os.path.join(os.path.dirname('model'), 'model','saved_model','model_gat_trained.pt'))\n",
    "    writer.close()\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "633ce58a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The MPS backend is supported on MacOS 12.3+.Current OS version can be queried using `sw_vers`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_gat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m, in \u001b[0;36mtrain_gat\u001b[0;34m(params_network, num_epochs, val_lapse, perform_test)\u001b[0m\n\u001b[1;32m      4\u001b[0m device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m time_start\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m model_gat\u001b[38;5;241m=\u001b[39m\u001b[43mGAT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_network\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams_network\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_nodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams_network\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_features_per_layer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams_network\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_heads_per_layer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(model_gat\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m) \u001b[38;5;66;03m# weight decay corresponds to the L2 penalty, which in the original implementation is chosen to the value we put here\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ENS/3eme_annee/Deep_learning/project/GAT_env/lib/python3.11/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ENS/3eme_annee/Deep_learning/project/GAT_env/lib/python3.11/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ENS/3eme_annee/Deep_learning/project/GAT_env/lib/python3.11/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ENS/3eme_annee/Deep_learning/project/GAT_env/lib/python3.11/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ENS/3eme_annee/Deep_learning/project/GAT_env/lib/python3.11/site-packages/torch/nn/modules/module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 662\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/Documents/ENS/3eme_annee/Deep_learning/project/GAT_env/lib/python3.11/site-packages/torch/nn/modules/module.py:985\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The MPS backend is supported on MacOS 12.3+.Current OS version can be queried using `sw_vers`"
     ]
    }
   ],
   "source": [
    "train_gat(params_network,num_epochs=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a8bca03",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'mps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmps\u001b[49m\u001b[38;5;241m.\u001b[39mis_available\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'mps'"
     ]
    }
   ],
   "source": [
    "torch.mps.is_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f8bff03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model/saved_model'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.path.dirname('model'), 'model','saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b60ca3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf730140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
